{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d1c564",
   "metadata": {},
   "source": [
    "# From existing notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52e50fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# shap\n",
    "import shap\n",
    "\n",
    "# plots and images\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e063dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train file: (11994, 92)\n",
      "Shape of test file: (2999, 92)\n",
      "Train cols: Index(['Unnamed: 0.1', 'Unnamed: 0', 'Type', 'Name', 'Age', 'Breed1_0',\n",
      "       'Breed1_1', 'Breed1_2', 'Breed1_3', 'Breed1_4', 'Breed1_5', 'Breed1_6',\n",
      "       'Breed1_7', 'Breed2_0', 'Breed2_1', 'Breed2_2', 'Breed2_3', 'Breed2_4',\n",
      "       'Breed2_5', 'Breed2_6', 'MaturitySize', 'FurLength', 'Vaccinated_1',\n",
      "       'Vaccinated_2', 'Vaccinated_3', 'Dewormed_1', 'Dewormed_2',\n",
      "       'Dewormed_3', 'Sterilized_1', 'Sterilized_2', 'Sterilized_3', 'Health',\n",
      "       'Quantity', 'Fee', 'StateID_0', 'StateID_1', 'StateID_2', 'StateID_3',\n",
      "       'VideoAmt', 'Description', 'PetID', 'PhotoAmt', 'AdoptionSpeed',\n",
      "       'vertex_xs', 'vertex_ys', 'bounding_confidences',\n",
      "       'bounding_importance_fracs', 'dominant_blues', 'dominant_greens',\n",
      "       'dominant_reds', 'dominant_pixel_fracs', 'dominant_scores',\n",
      "       'label_descriptions', 'label_scores', 'doc_scores', 'doc_magnitudes',\n",
      "       'languages', 'StateName', 'state_population', 'median_state_income',\n",
      "       'Invalid_name', 'IsTopRescuer', 'RescuerCount', 'Fee_binary',\n",
      "       'Fee_bin_1', 'Fee_bin_2', 'Fee_bin_3', 'Fee_bin_4', 'Quantity_binary',\n",
      "       'Quantity_bin_1', 'Quantity_bin_2', 'Quantity_bin_3', 'Age_guessed',\n",
      "       'Age_bin_1', 'Age_bin_2', 'Age_bin_3', 'Age_bin_4', 'Age_bin_5',\n",
      "       'has_Video', 'has_Photo', 'isMale', 'isFemale', 'Black', 'Brown',\n",
      "       'Golden', 'Yellow', 'Cream', 'Gray', 'White', 'ColorCount',\n",
      "       'isGeneric_Breed', 'has_descriptoin'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load train file\n",
    "train_file = 'split_train_clean.csv' \n",
    "train = pd.read_csv(train_file)\n",
    "\n",
    "# Load test file\n",
    "test_file = 'split_test_clean.csv'\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "print(f'Shape of train file: {train.shape}')\n",
    "print(f'Shape of test file: {test.shape}')\n",
    "print(f'Train cols: {train.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bc9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of missing data points pre: 1441\n",
      " Number of missing data points post: 0\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "print(f' Number of missing data points pre: {sum(train.isna().sum())}')\n",
    "\n",
    "# Drop NAs\n",
    "train = train.dropna()\n",
    "\n",
    "print(f' Number of missing data points post: {sum(train.isna().sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28f638d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2930\n",
       "3    2820\n",
       "0    2478\n",
       "2    2360\n",
       "Name: AdoptionSpeed, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group 0 & 1 and relabel target variable in train and test\n",
    "relabel_dict = { 0:0, 1:0, 2:1, 3:2, 4:3}\n",
    "train['AdoptionSpeed'] = train['AdoptionSpeed'].map(relabel_dict)\n",
    "test['AdoptionSpeed'] = test['AdoptionSpeed'].map(relabel_dict)\n",
    "\n",
    "# Value counts of re-labeled AdoptionSpeed\n",
    "train['AdoptionSpeed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bb46520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After downsampling, our data contains 9440 pets and 92 columns\n"
     ]
    }
   ],
   "source": [
    "group_size = 2360\n",
    "\n",
    "# Downsample so all 4 categories have equal reviews in train\n",
    "temp_0 = train[train.AdoptionSpeed.eq(0)].sample(\n",
    "    n=group_size,\n",
    "    replace=False)\n",
    "\n",
    "temp_1 = train[train.AdoptionSpeed.eq(1)].sample(\n",
    "    n=group_size,\n",
    "    replace=False)\n",
    "\n",
    "temp_2 = train[train.AdoptionSpeed.eq(2)].sample(\n",
    "    n=group_size,\n",
    "    replace=False)\n",
    "\n",
    "temp_3 = train[train.AdoptionSpeed.eq(3)].sample(\n",
    "    n=group_size,\n",
    "    replace=False)\n",
    "\n",
    "train_bal = pd.concat(\n",
    "    [temp_0, temp_1, temp_2, temp_3],\n",
    "    axis=0)\n",
    "\n",
    "# shuffle df_balanced\n",
    "train_bal.sample(frac=1) # frac=1 retains all the data\n",
    "train_bal.reset_index(drop=True, inplace=True) # reset index\n",
    "\n",
    "print('After downsampling, our data contains', train_bal.shape[0], 'pets and', train_bal.shape[1], 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e4b8e",
   "metadata": {},
   "source": [
    "## previous model: only use Description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d22361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 23:13:40.711125: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "Epoch 1/10\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.3831 - accuracy: 0.2704 - val_loss: 1.3762 - val_accuracy: 0.3030\n",
      "Epoch 2/10\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 1.3473 - accuracy: 0.3602 - val_loss: 1.3485 - val_accuracy: 0.3443\n",
      "Epoch 3/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.2594 - accuracy: 0.4452 - val_loss: 1.3363 - val_accuracy: 0.3459\n",
      "Epoch 4/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.1506 - accuracy: 0.5075 - val_loss: 1.3618 - val_accuracy: 0.3607\n",
      "Epoch 5/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.0492 - accuracy: 0.5616 - val_loss: 1.3959 - val_accuracy: 0.3490\n",
      "Epoch 6/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.9577 - accuracy: 0.6103 - val_loss: 1.4539 - val_accuracy: 0.3517\n",
      "Epoch 7/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.8735 - accuracy: 0.6568 - val_loss: 1.5187 - val_accuracy: 0.3475\n",
      "Epoch 8/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7998 - accuracy: 0.6986 - val_loss: 1.5887 - val_accuracy: 0.3469\n",
      "Epoch 9/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7325 - accuracy: 0.7288 - val_loss: 1.6754 - val_accuracy: 0.3464\n",
      "Epoch 10/10\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6720 - accuracy: 0.7560 - val_loss: 1.7468 - val_accuracy: 0.3506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_bal['Description'], train_bal['AdoptionSpeed'], test_size=0.2, random_state=42)\n",
    "\n",
    "vocab_size = 10000\n",
    "max_sequence_length = 50\n",
    "embedding_dim = 64\n",
    "\n",
    "#initialize TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length)\n",
    "\n",
    "#fit it on the training data\n",
    "vectorize_layer.adapt(X_train.to_numpy())\n",
    "\n",
    "\n",
    "#vectorize the data\n",
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)\n",
    "\n",
    "\n",
    "X_train = vectorize_text(X_train)\n",
    "X_val = vectorize_text(X_val)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_sequence_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')  # 4 classes in 'AdoptionSpeed'\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5e4093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 4ms/step - loss: 1.7529 - accuracy: 0.3458\n",
      "Test Loss: 1.752939224243164\n",
      "Test Accuracy: 0.3457944095134735\n"
     ]
    }
   ],
   "source": [
    "# apply the model to test set \n",
    "test = test.dropna(subset=['Description'])\n",
    "\n",
    "\n",
    "X_test = test['Description']\n",
    "Y_test = test['AdoptionSpeed']\n",
    "\n",
    "X_test = vectorize_text(X_test)\n",
    "\n",
    "# evaluate the model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815ca30",
   "metadata": {},
   "source": [
    "## now try with all other numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69575fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9189cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9440, 85)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns \n",
    "\n",
    "\n",
    "\n",
    "drop_cols = ['Unnamed: 0.1', 'Unnamed: 0', 'Name',\n",
    "             'PetID','label_descriptions','StateName','languages'] #val_accuracy ~0.3\n",
    "\n",
    "# drop_cols = ['Unnamed: 0.1', 'Unnamed: 0', 'Name',\n",
    "#              'PetID', 'vertex_xs', 'vertex_ys', 'bounding_confidences',\n",
    "#              'bounding_importance_fracs', 'dominant_blues', 'dominant_greens',\n",
    "#              'dominant_reds', 'dominant_pixel_fracs','label_descriptions','StateName','languages']\n",
    "\n",
    "# drop_cols = ['Unnamed: 0.1', 'Unnamed: 0', 'Name','Breed1_0','PetID',\n",
    "#        'Breed1_1', 'Breed1_2', 'Breed1_3', 'Breed1_4', 'Breed1_5', 'Breed1_6',\n",
    "#        'Breed1_7', 'Breed2_0', 'Breed2_1', 'Breed2_2', 'Breed2_3', 'Breed2_4',\n",
    "#        'Breed2_5', 'Breed2_6', 'Vaccinated_1',\n",
    "#        'Vaccinated_2', 'Vaccinated_3', 'Dewormed_1', 'Dewormed_2',\n",
    "#        'Dewormed_3', 'Sterilized_1', 'Sterilized_2', 'Sterilized_3', 'vertex_xs', 'vertex_ys', 'bounding_confidences',\n",
    "#        'bounding_importance_fracs', 'dominant_blues', 'dominant_greens',\n",
    "#        'dominant_reds', 'dominant_pixel_fracs', 'dominant_scores',\n",
    "#        'label_descriptions', 'label_scores', 'doc_scores', 'doc_magnitudes',\n",
    "#        'languages', 'StateName', 'Invalid_name', 'IsTopRescuer', 'RescuerCount', 'Fee_binary',\n",
    "#        'Fee_bin_1', 'Fee_bin_2', 'Fee_bin_3', 'Fee_bin_4', 'Quantity_binary',\n",
    "#        'Quantity_bin_1', 'Quantity_bin_2', 'Quantity_bin_3', 'Age_guessed',\n",
    "#        'Age_bin_1', 'Age_bin_2', 'Age_bin_3', 'Age_bin_4', 'Age_bin_5'] #produced very bad results val_accuracy 0.05\n",
    "    \n",
    "\n",
    "\n",
    "useful_train = train_bal.drop(columns=drop_cols).copy()\n",
    "\n",
    "useful_train.shape \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2504337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (9440, 84)\n",
      "Shape of y_train: (9440,)\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "y_train = useful_train['AdoptionSpeed']\n",
    "X_train = useful_train.drop('AdoptionSpeed', axis=1)\n",
    "\n",
    "\n",
    "# # Test data\n",
    "# y_test = test['AdoptionSpeed']\n",
    "# X_test = test.drop(columns = drop_cols,axis = 1, inplace =True)\n",
    "# X_test = test.drop('AdoptionSpeed', axis=1)\n",
    "\n",
    "# Print statement\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6829513",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "MAXLEN = 100\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train['Description'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train['Description'])\n",
    "\n",
    "X_train_description = pad_sequences(sequences, maxlen=MAXLEN)\n",
    "\n",
    "#standardize the numerical features\n",
    "#need to do the same for test later \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop('Description',axis = 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c3f0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ffnn_with_embedding(num_features, num_words, embedding_dim, maxlen, learning_rate=0.001):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "  \n",
    "    input_text = tf.keras.Input(shape=(maxlen,), name='Input_Text')\n",
    "    input_num = tf.keras.Input(shape=(num_features,), name='Input_Numeric')\n",
    "    \n",
    "    #embedding layer for text\n",
    "    x_text = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=maxlen)(input_text)\n",
    "    x_text = tf.keras.layers.Flatten()(x_text)\n",
    "\n",
    "    #concatenate all inputs\n",
    "    x = tf.keras.layers.Concatenate()([x_text, input_num])\n",
    "\n",
    "    x = tf.keras.layers.Dense(units=32,activation='relu', name='fc_1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x) #regularization\n",
    "\n",
    "    x = tf.keras.layers.Dense(units=16,activation='relu', name='fc_2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x) #regularization\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "          units=4,  \n",
    "          use_bias=False,\n",
    "          activation='softmax',\n",
    "          name='Output')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_text, input_num], outputs=output)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "545e743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "model = build_ffnn_with_embedding(num_features=X_train_scaled.shape[1], num_words=NUM_WORDS, embedding_dim=EMBEDDING_DIM, maxlen=MAXLEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0624207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "531/531 [==============================] - 4s 8ms/step - loss: 1.3872 - accuracy: 0.2516 - val_loss: 1.3739 - val_accuracy: 0.3485\n",
      "Epoch 2/15\n",
      "531/531 [==============================] - 3s 7ms/step - loss: 1.3759 - accuracy: 0.2985 - val_loss: 1.3925 - val_accuracy: 0.2892\n",
      "Epoch 3/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3660 - accuracy: 0.3227 - val_loss: 1.4082 - val_accuracy: 0.2701\n",
      "Epoch 4/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3568 - accuracy: 0.3389 - val_loss: 1.4256 - val_accuracy: 0.2765\n",
      "Epoch 5/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3472 - accuracy: 0.3459 - val_loss: 1.4410 - val_accuracy: 0.2712\n",
      "Epoch 6/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3378 - accuracy: 0.3515 - val_loss: 1.4584 - val_accuracy: 0.2744\n",
      "Epoch 7/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3299 - accuracy: 0.3619 - val_loss: 1.4697 - val_accuracy: 0.2712\n",
      "Epoch 8/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3208 - accuracy: 0.3675 - val_loss: 1.4829 - val_accuracy: 0.2691\n",
      "Epoch 9/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3143 - accuracy: 0.3629 - val_loss: 1.4915 - val_accuracy: 0.2712\n",
      "Epoch 10/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3055 - accuracy: 0.3762 - val_loss: 1.4962 - val_accuracy: 0.2818\n",
      "Epoch 11/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.3004 - accuracy: 0.3745 - val_loss: 1.5025 - val_accuracy: 0.2892\n",
      "Epoch 12/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.2951 - accuracy: 0.3824 - val_loss: 1.5052 - val_accuracy: 0.2987\n",
      "Epoch 13/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.2884 - accuracy: 0.3843 - val_loss: 1.5081 - val_accuracy: 0.3072\n",
      "Epoch 14/15\n",
      "531/531 [==============================] - 4s 7ms/step - loss: 1.2859 - accuracy: 0.3847 - val_loss: 1.5081 - val_accuracy: 0.3189\n",
      "Epoch 15/15\n",
      "531/531 [==============================] - 3s 7ms/step - loss: 1.2830 - accuracy: 0.3824 - val_loss: 1.5065 - val_accuracy: 0.3252\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=[X_train_description, X_train_scaled],\n",
    "    y=y_train,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79db72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
